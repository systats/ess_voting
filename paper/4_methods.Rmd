---
output: pdf_document
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = F, warning = F, error = F, message = F)
```

```{r, echo = F}
pacman::p_load(dplyr, ggplot2, ggthemes, forcats, tidyr, broom, stringr, kableExtra, knitr)
ggplot2::theme_set(ggthemes::theme_few())
```

```{r}
dt <- get(load("data/data_final.Rdata"))
ches_clust <- get(load("data/ches_clust.Rdata"))
```



The following section lays the groundwork for testing our set of hypotheses. We start with a detailed description of the datasets, followed by Model based clustering to infer the dependent variable labels, dimensionality reduction (PCA) and finally the statistical models deployed. 

## Data and Variables

We collected data from different sources and abstraction levels. The datasets involved are briefly introduced below:


* **European Social Survey** [@ess2016]. The ESS is a multi-country scientific survey conducted every two years since 2002 by scientists from several European countries. It aims to understand the changing attitudes and values in Europe and how institutions are changing. It comprises a wide range of European social indicators[@ess08]. We retrieved the latest data available for each country and year. This micro dataset is of core interest to which all following datasets are merged. We chose the following variables:
* **Chapel Hill Expert Survey** [@ches2014]: Since 1999 CHES provides party positioning scores on European integration, ideology and policy issues for national parties in a variety of European countries. This data set is used for clustering parties according to their similarity on populist indicators.
* **Varieties of Democracy** [@vdem2017]: V-DEM aims to produce indicators of Democracy that are multi-dimensional and disaggregated. This modern framework reflects the complexity of the concept of democracy and captures seven high-level principles of democracy: electoral, liberal, participatory, deliberative, egalitarian, majoritarian and consensual.
* **Human Development Index** [@HDI]: Brought by the UN Human Development Programm, HDI emphasizes people's capabilities as ultimate criteria for assessing the development of a country, not economic growth alone. This variable approximates modernization on a macro level.
* **World Bank Development Indicators** [@WDI]: An international effort to measure state performance and to reduce poverty through informative data analysis.


Our variables are listed in table 1 which provides the original variable names as well as a detailed description. A small amount of input vectors are the result of data transformations which will be discussed in the following sections.


```{r vars, fig.pos="ht!"}
vars <- rbind(
  c("Gender", "gndr", "", "1 Female - 0 Male.", "ESS"),
  c("Income", "hinctnta", "Household's total net income", "1 (1st decentile) - 10 (10th decentile)", "ESS"),
  c("Pol. Interest", "polintr", "Political interest", "1 (not) - 10 (very)", "ESS"),
  c("Left-Right", "lrscale", "Self-positioning on left-right scale.", "1 (Left) - 10 (Right", "ESS"),
  c("Religiosity", "rlgatnd", "Attendance of religious services.", "1 (Every day) - 7 (Never)", "ESS"),
  c("Trust Parl.", "trstprl", "Trust in parliament", "1 (not) - 10 (very)", "ESS"),
  c("Trust Just.", "trstlgl", "Trust in the legal system", "", "ESS"),
  c("Trust Police", "trstplc", "Trust in the police?", "", "ESS"),
  c("Trust Polit.", "trstplt", "Trust in the politicians?", "", "ESS"),
  c("Trust Parties", "trstprt", "Trust in political parties?", "", "ESS"),
  c("Trust EU", "trstep", "Trust in the European Parliament?", "", "ESS"),
  c("Imm. Economy", "imbgeco", " Immigration is ...", "1 (Bad for the economy) - 10 (Good for the economy)", "ESS"),
  c("Imm. Equal", "imsmetn", "should allow people of the same race or ethnic group to come and live here?", "1 (Allow many to come and live here) - 4 (Allow none).", "ESS"),
  c("Imm. Diff.", "imdfetn", "How about people of a different race or ethnic group?", "", "ESS"),
  c("Imm. Poor", "impcntr", "How about people from the poorer countries outside Europe?", "", "ESS"),
  c("Anti-Elite", "ANTIELITE", "Salience of anti-establishment and anti-elite rhetoric.", "0 (Not important at all) - 10 (Extremely important)", "CHES"),
  c("Anti-EU", "POSITION", "Orientation of the party leadership towards European integration.", "1 (Strongly opposed) - 7 (Strongly in favor)", "CHES"),
  c("Left-Right", "LRGEN", "Position of the party in terms of its overall ideological stance.", "0 (Extreme left) - 5 (Center) - 10 (Extreme right)", "CHES"),
  c("Galtan", "GALTAN", "party position on democratic freedoms and rights. “Libertarian” or “postmaterialist” vs. “Traditional” or “authoritarian”", "0 (Libertarian/Postmaterialist) - 10 (Traditional/Authoritarian)", "CHES"),
  c("ORI", "v2xdd_i_or", "Obligatory referendum index", "", "VDEM"),
  c("HDI", "hdi", "Human development Index.", "", "UN"),
  c("% of Refuggees", "SM.POP.REFG", "Refugee population by country or territory of asylum.", "Measured in percent of total population", "WDI")
) %>% 
  as_tibble %>%
  rename(Var = V1, Original = V2, Description = V3, Attributes = V4, Source = V5)


library(knitr)
library(kableExtra)
vars %>% 
  knitr::kable(format = "latex", booktabs = T, caption = "Overview of Variables") %>%
  column_spec(1, bold = T) %>%
  column_spec(3, width = "15em") %>%
  column_spec(4, width = "15em") %>%
  kable_styling(latex_options = c("scale_down", "hold_position")) #%>% #font_size = 9, 
  #add_footnote(c("Note: ..."), notation = "number")
```

\clearpage

## Model Based Clustering

Voting for a right-wing party is often approximated by left-right scales or is time-intensively coded. Instead we employ model based clustering to estimate a party's label from its multi-dimensional positioning according to the CHES party indicators. We adopt Cas Mudde’s clear minimalist definition of populism to identify those core features which are shared by all sub-types of populism [@mudde2007populist]. In line with this definition, we suggest that populist parties are primarily shaped by their degree of anti-establishment attitudes, left-right positioning as well as their opposition to modernization. 

This multi-dimensional classification problem is best approached by model-based hierarchical clustering [@mclust]. This tool set has reached political sciences and ever since has been praised for estimating meaningful clusters on high-dimensional data [@mclust_pol1; @mclust_pol2]. Model-based clustering assumes that the data generating process is driven by a mixture of underlying probability distributions in which each component represents a different cluster. Consequently each sub-population is separately estimated and summarized by a mixture of these sub-populations. Therefore the density tends to be centered at the multidimensional centroids ($\mu_k$). They increase through geometric features (shape, volume, orientation) of the clusters being determined by the parameters of the covariance matrices $\Sigma_k$, which may also induce cross-cluster conditions [@mclust_pol1]. In a nutshell, the Gaussian Finite Normal Mixture model assumes a d-dimensional data set $y_1, \dots , y_n$ to calculate G components with the likelihood

$$\ell_{MIX} (\theta_1, \dots, \theta_G|y) = \prod^n_{i=1}\sum^G_{k=1} \tau_kf_k(y_i|\theta_k) $$


where $f_k$ is the density of the $\theta_k$ mixture parameters. Unlike traditional methods, model-based clustering uses a soft assignment and calculates $\tau_k$ which represents the probability of a given observation belonging to the k component [@mclust_pol1]. Unsupervised machine learning algorithms are often criticized for introducing bias by hyper parameter settings. This is addressed by estimating a grid of different models and hyper parameter constellations ^[The model based clustering is conducted with the `mclust` package [@mclust]]. Bayesian Information Criterion (BIC) is provided to pick the most useful model [@mclust]. The metric is penalized for the complexity of the model to ensure Occam's Razor law of parsimony. 


\begin{figure}[ht!]
\centering
\label{model_cluster}
\includegraphics[width=0.9\linewidth]{images/gg_mc_bic.pdf} 
\caption{BIC Model Selection for Mixture Models on CHES Data}
\end{figure}


Figure 1 indicates a three-component mixture with covariances having different volume, shape, and orientation (VVV) ^[The estimated grid models are represented by identifiers. The first Letter refers to volume, the second to shape and the third to orientation. E stands for “equal”, V for “variable” and I for “coordinate axes” [@kassambara2017practical]. Following Fraley and Raftery (2010), the grid component structures are labeled using: EII=spherical, equal volume; EEI=diagonal, equal volume and shape; EVI=diagonal, equal volume, varying shape; EEE=ellipsoidal, equal volume, shape, and orientation; VEV=ellipsoidal, equal shape; VII=spherical, unequal volume; VEI=diagonal, varying volume, equal shape; VVI=diagonal, varying volume and shape; EEV=ellipsoidal, equal volume and equal shape; VVV=ellipsoidal, varying volume, shape, and orientation.]. Following Milligan and Cooper’s (1988) we standardized the variables which ensures a more consistently superior recovery of the underlying cluster structure. Principal Component Analysis is then used to better understand the multi-dimensional problem, by reconstructing only two dimensions. Figure 2 shows the generic distinction between Left-Populist, Establishment and Right-Populist Parties.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/gg_mc_cluster1.pdf} \label{cluster_dim}
\caption{Classification and Cluster Boundaries}
\end{figure}


In order to check the consistency of the clusters, the variable means and standard deviations are calculated for each cluster (Table 2).  We can see that Anti-Establishment positions are shared by right and left populist parties. On the dimensions we can observe a antagonistic distinction between both. The cluster vector is finally attached to the data set. The binary dependent variable *Voting Right Populists* is obtained by one-hot-encoding of the trichotomous cluster vector. 


```{r}
cluster_means <- ches_clust %>%
  group_by(cluster) %>%
  select(antielite_salience, eu_position, civlib_laworder, galtan) %>% 
  summarise_all(mean) %>%
  mutate(cluster = NULL) %>%
  mutate_all(round, 2) %>%
  t

cluster_sd <- ches_clust %>%
  group_by(cluster) %>%
  select(antielite_salience, eu_position, civlib_laworder, galtan) %>% 
  summarise_all(sd) %>%
  mutate(cluster = NULL) %>%
  mutate_all(round, 2) %>%
  t

rn <- rownames(cluster_means)
cluster_means <- cbind(rn, cluster_means) %>% as_tibble()
cluster_sd <- cbind(rn, cluster_sd) %>% as_tibble() 
  
colnames(cluster_means) <- c("Var","Establishment", "Left Populist", "Right Populist")

cluster_sd <- cluster_sd %>%
  select(2:4) %>%
  mutate_all(function(x) paste0("(", x, ")")) %>%
  data.frame(Var = rn, ., stringsAsFactors = F)

colnames(cluster_sd) <- c("Var","Establishment", "Left Populist", "Right Populist")

cluster_all <- rbind(cluster_means, cluster_sd) %>%
  arrange(Var) 
cluster_all$Var[stringr::str_detect(cluster_all$Establishment, "^\\(")] <- ""

cluster_all[1,1] <- "Anti-Elite"
cluster_all[3,1] <- "Left-Right"
cluster_all[5,1] <- "Anti-EU"
cluster_all[7,1] <- "Galtan"

cluster_all %>% 
  knitr::kable(format = "latex", booktabs = T, caption = "Cluster Means and Standard Deviations for Populism Indicators") %>%
  column_spec(1, bold=T) %>%
  kable_styling(font_size = 10, latex_options = c("hold_position")) %>%
  add_footnote(c("Note: Standard deviations are in parentheses"), notation = "number")
```


```{r cluster_means, echo = F, eval = F}
ches_parties <- ches_clust %>%
  mutate(party_name = paste0(party_name, " (", cname, ")")) %>%
  filter(cname %in% c("ger", "fra")) %>%
  select(party_name, cluster) %>%
  group_by(cluster) %>%
  summarise(Parties = paste(party_name, collapse = "; ")) %>%
  ungroup %>%
  rename(Cluster = cluster)

ches_parties %>% 
  knitr::kable(format = "latex", booktabs = T, caption = "Party Cluster Membership") %>%
  column_spec(1, bold=T) %>%
  column_spec(2, width = "25em") %>%
  kable_styling(font_size = 10, latex_options = c("hold_position")) %>%
  add_footnote(c("Note: ..."), notation = "number")
```


\clearpage

## Principal Component Analysis

Principal Component Analysis (PCA), another unsupervised machine learning approach, is now used to reduce the dimensions of two item batteries and is broken down to a more parsimonious model size. We are generally interested in the lowest number of dimension/component that contributes to most of the variability in the underlining data structure. The formal model for the first principal component of a dataset, is the linear combination of its features 

$$Z_i = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1}X_p$$

that has the largest variance. The first principal component loading vector, with elements $\phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1}$ is normalized, which means that $\sum^p_{j=1} \phi^2_{j1} = 1$. To calculate these loadings, we must find the vector that maximizes the variance. By the use of techniques from linear algebra, it can be shown that the eigenvector corresponding to the largest eigenvalue of the covariance matrix is the set of loadings that explains the greatest proportion of the variability. In comparison to Factor Analysis or SEM, this method does not depend on model assumptions or multivariate error distributions. Despite all dimensionality, reduction methods have different backgrounds and purposes and often yield similar results. For construct validation we recommend factor analysis or any other framework that provides hypothesis testing which is not needed in our case. 


### Trust Items

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/trust_cor.png} 
\caption{Bivariate Scatter Plots for Trust Items}
\end{figure}

First we explore the dimensionality of the given trust items by applying standard pairwise scatter plots for each variable combination. The visual inspection confirms a strong linear association between all variables and the diagonal histograms approximate a normal distribution with a heavy tail towards 0. People are especially critical towards political institutions, but have more overall confidence in the police. Furthermore we can see that *Trust EU* has lower correlations compared to the rest of the items. Therefore *Trust EU* is excluded from the PCA and separately included in the regressions models.  

The next plot illustrates a two-dimensional representation of the data which captures most of the information in a 2D subspace. The first two components together contribute nearly 80% of the total variance. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\linewidth]{images/pca1_vis.pdf} 
\caption{First and Second PC Dimension for Trust}
\end{figure}

\begin{figure}[ht!]
\centering
\label{trust_eval}
\includegraphics[width=0.9\linewidth]{images/trust_eval.pdf} 
\caption{Scree Plot and Component Contribution by Variable}
\end{figure}

As each principal component vector defines a direction in the feature space and all arrows in figure 4 point to the same direction, we can be confident to approximate one trust dimension. This notion is supported by the scree plot capturing 66,7% of the total variance by the first dimension. Furthermore we can see that the trust items for political institutions do somewhat differ in their contribution to the first PC due to different reasons. The European Parliament is often seen as disconnected from people and the police is always highly appreciated by everyone which supports the notion that *Trust EU* should be considered desperately. The first PC scores are stored for later analysis as *Pol. Trust (PC)*. 


### Immigration Items

\begin{figure}[ht!]
\label{imm_cor}
\centering
\includegraphics[width=0.9\linewidth]{images/imm_cor.png} 
\caption{Bivariate Scatter Plots for Immigration Items}
\end{figure}

The same procedure is applied to the ESS immigration items. But first we, again, explore the correlation of the given data by pairwise scatter plots for each variable combination in figure 6. The visual inspection confirms a strong linear association between all variables but seem to be more noisy than the trust items before. The diagonal histograms approximate a normal distribution, despite the fact that three out of four variables only have four levels. This Likert scale is assumed to be equally distant to be suitable for PCA. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.5\linewidth]{images/pca2_vis.pdf} 
\caption{First and Second PC Dimension for Immigration}
\end{figure}

The two-dimensional representation of the immigration items also proposes a strong one PC solution but points to a minor second dimension entirely rooted in imm_econ. Here we are interested in the first two PCs that capture together over 70% of the total variance. The scree plot and contribution plot support these findings. The final principal components are stored as *Cult. Immigration (PC)* and *Econ. Immigration (PC)*


\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/imm_eval.pdf} 
\caption{Scree Plot and Component Contribution by Variable}
\end{figure}


## Statistical Models

As already explained in section 4.1 and the dependent variable $y_i$ is labeled '1' for voters of right-wing parties estimated by model based-clustering. 

$$
y_i = \left\{
\begin{array}{l}
1:\text{ if voted for a right populist party}\\
0:\text{ if not}
\end{array}\right.
$$

This binary data structure is best fitted by generalized linear models (GLMs) with Binomial response function (Bernoulli). Within the scope of this paper, we built different logistic regression models with varying size amount of complexity. First, we introduce simple logistic regression. Second, we augment logistic regression with a hierarchical representation to better capture the nested data generation process of the ESS micro/country data structure [@gelman07].


The first simple logistic model predicts the mean log-odds of a respondent voting right-wing through a linear combination of an intercept and a slope - the latter allowing to quantify the effect of each covariate. Finally the inverse link function ($logit^{-1}$) non-linearly transforms the outcome into new vector space $\in(0,1)$


\begin{align}
  y_i & \sim \text{Binomial}(n_i,p_i)\\
  logit^{-1}(p_i) & = \alpha + \beta x_{i}
\end{align}


This complete pooling architecture is separately deployed to each country dataset and summarized accordingly to explore country-specific model characteristics as well as deficiencies. The fitting function is taken from the core R `stats` functions. 

The simple version of the GLM assumes independence of each individual observation which produces in a less severe scenario downwards biased parameter standard errors (SE). By deploying random effects with context level predictors we are able to model country-specific heterogeneity and group level standard errors [@steenbergen02]. The formal multilevel logistic regression has the formula 


\begin{align}
  y_i &\sim \text{Binomial}(n_i,p_i)\\
  logit^{-1}(p_i) & = \alpha_{j} + \beta x_{i}\\
  \alpha_{j} & \sim N(\mu_\alpha, \sigma^2_{\alpha})
\end{align}

The group intercepts $\alpha_j$ are assumed to be an independent and identically distributed random variable with hyperprior grand mean and equal variance. This population distribution is partially fitted by pooling its sub-population intercepts (share information) and used to refit or regularize the parameters towards 0. Thereby we compromise between complete pooling and no pooling, in order to balance bias and variance (trade-off). In sum we avoid fixed effects for nested data to control model complexity and prevent overfitting.

<!-- ### Bayesian Approach -->

<!-- These model have substantive advantages for our analysis as -->

<!-- * estimates are penalized towards to a plausible parameter space -->
<!-- * prior knowledge can be incorporated -->
<!-- * do not link evidence to p-values but rather propagate the uncertainty of the model to the posterior distribution so we can inspect it.  -->
<!-- * ... and many more.  -->

<!-- Nomral byaesian logistic regression -->

<!-- \begin{align} -->
<!--   y_i &\sim \text{Binomial}(n_i,p_i)\\ -->
<!--   logit^{-1}(p_i) &= \alpha + \beta x_{i} \in(0,1)\\ -->
<!--   \alpha &\sim N(0, 10)\\ -->
<!--   \beta &\sim N(0, 10) -->
<!-- \end{align} -->

<!-- Varying Intercept ... -->



<!-- \begin{align} -->
<!--   y_i & \sim Binomial(n_i,p_i)\\ -->
<!--   logit^{-1}(p_i) & = \alpha_j + \beta x_{i} \in(0,1)\\ -->
<!--   \alpha_j & \sim N(\alpha, \sigma)\\ -->
<!--   \alpha & \sim N(0, 10)\\ -->
<!--   \sigma &\sim \text{HalfCauchy}(0, 10)\\ -->
<!--   \beta & \sim N(0, 10) -->
<!-- \end{align} -->




```{r, eval = F, fig.cap="Number of Survey Responses by Country", fig.pos="ht!"}
dt %>%
  group_by(country, round, round_year) %>%
  tally %>%
  ungroup() %>%
  mutate(country = fct_reorder(country, n)) %>%
  mutate(label = paste0(round, " (", round_year, ")")) %>%
  ggplot(aes(country, n, fill = label)) +
  geom_bar(stat = "identity") +
  scale_fill_grey(start = .8, end = .4) +
  coord_flip() +
  labs(x = "", y = "")
```


<!-- $$\pi(y,x|\theta) = \pi(y|x, \theta) $$ -->
<!-- this cancelles out to  -->
<!-- $$\pi(y, x|\theta) \approx \pi(y|x, \theta) $$ -->
<!-- now we plug in a binomial distribution.  -->
<!-- $$\pi(y|x, \theta) = Bin(y|f(x, \theta), N)$$ -->