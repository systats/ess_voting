---
output: pdf_document
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = F, warning = F, error = F, message = F)
```

```{r, echo = F}
pacman::p_load(dplyr, ggplot2, ggthemes, forcats, tidyr, broom, stringr, kableExtra, knitr)
ggplot2::theme_set(ggthemes::theme_few())
```

```{r}
dt <- get(load("data/data_final.Rdata"))
ches_clust <- get(load("data/ches_clust.Rdata"))
```


## Research Design

Different data sources data set were used:

* **European Social Survey** [@ess2016]. The ESS is a multi-country scientific survey conducted every two years since 2002 by scientists in several European countries. The objectives of the ESS are to understand the changing attitudes and values in Europe, to explain how European institutions are changing and to develop a range of European social indicators and values [@ess08]. The data for the present analysis were collected from different survey waves the latest for each country.
* **Chapel Hill Expert Survey** [@ches2014]: Since 1999 CHES provides party positioning scores on European integration, ideology and policy issues for national parties in a variety of European countries. The more recent survey waves also comprise questions on non-EU policy issues, such as immigration, redistribution, decentralization, and environmental policy.
* **Varieties of Democracy** [@vdem2017]: V-DEM aims to transperently produce Indicators of Democracy that are multidimensional and disaggregated. This modern framework reflects the complexity of the concept of democracy and captures seven high-level principles of democracy: electoral, liberal, participatory, deliberative, egalitarian, majoritarian and consensual, and collects data to measure these principles. 

The dependent variable from our final model is the populism cluster infered from Ches party data that has been matched and merged to the ESS micro data on voting a particular party. Variable Description ... 

* [summary stats](https://github.com/dcomtois/summarytools)


\clearpage

## Model Based Clustering

Voting for a right wing party is often approximated by left-right scales or is time-intensiv coded. This analysis adopts Cas Mudde’s clear minimalist definition of populism to identify core features that all sub types of populism have in common. In line with this definition, we suggest that populist parties are primarily shaped by their degree of anti-establishment attitudes as well as their opposition to globalization. Subsequently, we propose to classify European populist parties along a progressive and traditionalist left-right dimension. Some CHES party indicators are part of the clustering even though they are not be present in the ESS micro data. 

<!-- - Left-Right -->
<!-- - Anti-Establishment -->
<!--     + antielite_salience  -->
<!--     + eu_position -->
<!-- - Progressivism vs Traditionalism -->
<!--     + civlib_laworder -->
<!--     + galtan  -->

This multidimensional classification problem is best approached by model-based hierarchical clustering [@mclust]. This tool set is already adopted in political science and praised for estimating meaningful clusters on high-dimensional data political science [@mclust_pol1; @mclust_pol2]. Model-based clustering assumes the data generating process to be driven by a mixture of underlying probability distributions in which each component represents a different cluster. Consequently each sub population is separately estimated and summarized by a mixture of these sub populations. Therefore the density tends to be centered at the multidimensional means ($\mu_k$) and increased by geometric features (shape, volume, orientation) of the clusters being determined by the parameters of the covariance matrices $\Sigma_k$, which may also induce cross-cluster conditions [@mclust_pol1]. In a nutshell the Gaussian Finite Normal Mixture model assumes a d-dimensional data set $y_1, \dots , y_n$ to calculate G components with the likelihood

$$\ell_{MIX} (\theta_1, \dots, \theta_G|y) = \prod^n_{i=1}\sum^G_{k=1} \tau_kf_k(y_i|\theta_k) $$


where $f_k$ is the density of the $\theta_k$ mixture parameters, unlike traditional methods model-based clustering uses a soft assignment and calculates $\tau_k$ that represents the probability of a given observation belonging to the k component [@mclust_pol1]. Unsupervised machine learning algorithms are often criticized for introducing bias by hyper parameter settings. `mclust` addresses this by estimating a grid of different models and hyper parameter constellations. Bayesian Information Criterion (BIC) is provided to pick the most useful model [@mclust]. The metric is penalized for the complexity of the model to ensure Osam Razor. 


\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/gg_mc_bic.pdf} 
\caption{BIC Model Selection for mixture models on the CHES party positioning data set. Following Fraley and Raftery (2010), the grid component structures are labeled using: EII=spherical, equal volume; EEI=diagonal, equal volume and shape; EVI=diagonal, equal volume, varying shape; EEE=ellipsoidal, equal volume, shape, and orientation; VEV=ellipsoidal, equal shape; VII=spherical, unequal volume; VEI=diagonal, varying volume, equal shape; VVI=diagonal, varying volume and shape; EEV=ellipsoidal, equal volume and equal shape; VVV=ellipsoidal, varying volume, shape, and orientation.}
\end{figure}

The estimated grid models are represented by the following identifiers: EII, VII, EEI, VEI, EVI, VVI, EEE, EEV, VEV and VVV. The first Letter refers to volume, the second to shape and the third to orientation. E stands for “equal”, V for “variable” and I for “coordinate axes” [@kassambara2017practical].


There is a clear indication of a four-component mixture with covariances having different shapes but the same volume and orientation (EVE) (VEV (ellipsoidal, equal shape)).

Following Milligan and Cooper’s (1988) finding that standardization by dividing
each variable by its range gives consistently superior recovery of the underlying cluster structure, all the variables are standardized by dividing by each variable’s range. PCA ... 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/gg_mc_cluster.pdf} 
\caption{Classification and Cluster Boundaries}
\end{figure}


Table X displays the characteristics of each cluster through mean and standard deviation. 


```{r}
cluster_means <- ches_clust %>%
  group_by(cluster) %>%
  select(antielite_salience, eu_position, civlib_laworder, galtan) %>% 
  summarise_all(mean) %>%
  mutate(cluster = NULL) %>%
  mutate_all(round, 2) %>%
  t

cluster_sd <- ches_clust %>%
  group_by(cluster) %>%
  select(antielite_salience, eu_position, civlib_laworder, galtan) %>% 
  summarise_all(sd) %>%
  mutate(cluster = NULL) %>%
  mutate_all(round, 2) %>%
  t

rn <- rownames(cluster_means)
cluster_means <- cbind(rn, cluster_means) %>% as_tibble()
cluster_sd <- cbind(rn, cluster_sd) %>% as_tibble() 
  
colnames(cluster_means) <- c("Var","Establishment", "Left Populist", "Right Populist")

cluster_sd <- cluster_sd %>%
  select(2:4) %>%
  mutate_all(function(x) paste0("(", x, ")")) %>%
  data.frame(Var = rn, ., stringsAsFactors = F)

colnames(cluster_sd) <- c("Var","Establishment", "Left Populist", "Right Populist")

cluster_all <- rbind(cluster_means, cluster_sd) %>%
  arrange(Var) 
cluster_all$Var[stringr::str_detect(cluster_all$Establishment, "^\\(")] <- ""

cluster_all %>% 
  knitr::kable(format = "latex", booktabs = T, caption = "Cluster means and standard deviations for populist indicators") %>%
  column_spec(1, bold=T) %>%
  kable_styling(font_size = 10) %>%
  add_footnote(c("Note: Standard deviations are in parentheses"), notation = "number")
```

To validate the clusters lets inspect the party classification for three countries. For checking the consitency of the clusters, the variable means are calculated by cluster in Table \@ref(tab:cluster_means).  The final cluster vector is attached to the data with name `cluster`.


```{r cluster_means}
ches_parties <- ches_clust %>%
  mutate(party_name = paste0(party_name, " (", cname, ")")) %>%
  filter(cname %in% c("ger", "fra")) %>%
  select(party_name, cluster) %>%
  group_by(cluster) %>%
  summarise(Parties = paste(party_name, collapse = "; ")) %>%
  ungroup %>%
  rename(Cluster = cluster)

ches_parties %>% 
  knitr::kable(format = "latex", booktabs = T, caption = "Party Cluster Membership") %>%
  column_spec(1, bold=T) %>%
  column_spec(2, width = "25em") %>%
  kable_styling(font_size = 10) %>%
  add_footnote(c("Note: ..."), notation = "number")
```




\clearpage

## Principal Component Analysis

Principal Component Analysis (PCA) another unsupervised machine learning approach is now used to reduce the dimensions of two item batteries to get a parsimonious model size. For both variables we are only interested in the first Dimension/Component that contributes most to the variability in the underlining data structure. The formal model for the first principal component of a data set is the linear combination its features 

$$Z_i = \phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1}X_p$$

that has the largest variance and where is the first principal component loading vector, with elements $\phi_{11} X_1 + \phi_{21} X_2 + \dots + \phi_{p1}$ are normalized, which means that $\sum^p_{j=1} \phi^2_{j1} = 1$. To calculate these loadings, we must find the vector that maximizes the variance. It can be shown using techniques from linear algebra that the eigenvector corresponding to the largest eigenvalue of the covariance matrix is the set of loadings that explains the greatest proportion of the variability. This method does not depend compared to Factor Analysis or SEM on model assumptions or multivariate error distributions. Despite all dimensionality reduction methods have different background and purposes they often yield similar results. But for construct validation I recommend factor analysis or any other framework that provides hypothesis testing.

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/trust_cor.png} 
\caption{Bivariate Scatter plots for item battery public trust}
\end{figure}


### Trust Items

First we explore the diemnionality of the given trust items by applying standard pairwise scatter plots for each variable combination. The visual inspection confirms a strong linear association between all variables and the diagonal histograms approximate a normal distribution with a heavy tail on 0, as people are especially critical against political institutions and put more overall confidence in the police. 

The next plot shows a two-dimensional representation of the data that captures most of the information in a lower-dimensional subspace. The First two components contribute together to nearly 80% of the total variance. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\linewidth]{images/pca1_vis.pdf} 
\caption{First and Second PC Dimensions for the Trust Items}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/trust_eval.pdf} 
\caption{scree plot and Component Contribution by Variable}
\end{figure}

As each principal component vector defines a direction in the feature space and all arrows in Figure X point to the same direction we can be confident to approximate one trust dimension. This notion is supported by the scree plot capturing 66,7% of the total Variance by the first dimension. We can further see that the trust items for political institutions do somewhat differ in their contribution to the first PC due to different reasons. The European Parliament is often seen as disconnected from people and the police is always highly appreciated by everyone. The first PC scores are stored for later analysis as `pc_trust`. 


### Immigration Items

\begin{figure}[ht!]
\label{imm_cor}
\centering
\includegraphics[width=0.9\linewidth]{images/imm_cor.png} 
\caption{Bivariate Scatter Plots for item battery immigration}
\end{figure}

Next we applied the same PCA for the Immigration Items. But first we explore again the nationality of the given data by standard pairwise scatter plots for each variable combination in figure \@ref(fig:imm_cor). The visual inspection again confirms a strong linear association between all variables but seem to be more noisy than the trust items before. The diagonal histograms approximate a normal distribution, despite the fact that three out of four variables have only 4 levels. This Likert scale is assumed to be equal-distant to be suitable for PCA. 

\begin{figure}[ht!]
\centering
\includegraphics[width=0.6\linewidth]{images/pca2_vis.pdf} 
\caption{PCA Dimensionality for the Immigration Items}
\end{figure}

The two-dimensional representation of the immigration items proposes again a strong one PC solution but points to a minor second dimension entirely rooted in imm_econ. We are only interested in the first PC that captures over 70% of the total variance. The scree plot and contribution plot support these findings. The final PC scores are stored for later analysis as `pc_imm`.


\begin{figure}[ht!]
\centering
\includegraphics[width=0.9\linewidth]{images/imm_eval.pdf} 
\caption{scree plot and Component Contribution by Variable}
\end{figure}

## Statistical Models

As detailed explained in section X and the dependent variable $y_i$ is labeled as 1 for voters of right wing populist parties estimated by model based-clustering. 

$$
y_i = \left\{
\begin{array}{l}
1:\text{ if voted for a right populist party}\\
0:\text{ if not}
\end{array}\right.
$$

This binary data structure is best fitted by generalized linear models (GLMs) with Binomial response function (Bernoulli). As part of this paper we built different logistic regression models coming from different statistcial backgrounds. First we used the common frequentist approach to statistical inference but rapidly augemented Bayesian models brought by the `rstanarm` package. Second we employed multilevel models both frequentist and bayesian to better capture the nested data generation process of the ESS micro/country data structure [@gelman07].


### Frequentist Approach

The first simple logistic model (frequentist) predicts the mean log-odds of a respndant voting right-populist, through a linear combination of an intercept and a slope the latter allowing to quantify the effect of each covariate. 

\begin{align}
  y_i & \sim \text{Binomial}(n_i,p_i)\\
  logit^{-1}(p_i) & = \alpha + \beta x_{i}
\end{align}

The inverse link function can be switched to the righ-hand side of the formula to non-linearlly transform the outcome.

This complete pooling archetecture is seperatly deployed to each country dataset and summerized accordingly to explor country-specific model defecencies. The fitting function is taken from the core `stats` function o R. 

The simple version of the GLM assumes independece of each individual observation which produces in a less severe szenario downwards biased parameter standard errors (SE). By deploying random effects with context level predictors we are able to model country-specific heterogenity and group level standard errors [@steenbergen02]. The formal multilevel logitic regresion has the formula 


\begin{align}
  y_i &\sim \text{Binomial}(n_i,p_i)\\
  logit^{-1}(p_i) & = \alpha_{j} + \beta x_{i} \in(0,1)\\
  \alpha_{j} & \sim N(\mu_\alpha, \sigma^2_{\alpha})
\end{align}

The group intercepts $\alpha_j$ are assumed to be a independent and identically distributed random variable with hyperprior grand mean and equal variance. This population distribution is partilly fitted by pooling its sub-pupulation intercepts (share information) and used to refit or reguralize the parameters towards 0. Thereby we compromise between complete pooling and no pooling that in order to balance bias and variance (trade-off). In sum we avoid fixed effects for nested data to control model complexity and prevent overfitting.


The models discussed so far are often seen in social sciences. Now we leave the world of asymptotic siginificance and turn to the Bayesian framework. 


### Bayesian Approach

These model have substantive advantages for our analysis as

* estimates are penalized towards to a plausible parameter space
* prior knowledge can be incorporated
* do not link evidence to p-values but rather propagate the uncertainty of the model to the posterior distribution so we can inspect it. 
* ... and many more. 

Nomral byaesian logistic regression

\begin{align}
  y_i &\sim \text{Binomial}(n_i,p_i)\\
  logit^{-1}(p_i) &= \alpha + \beta x_{i} \in(0,1)\\
  \alpha &\sim N(0, 10)\\
  \beta &\sim N(0, 10)
\end{align}

Varying Intercept ...



\begin{align}
  y_i & \sim Binomial(n_i,p_i)\\
  logit^{-1}(p_i) & = \alpha_j + \beta x_{i} \in(0,1)\\
  \alpha_j & \sim N(\alpha, \sigma)\\
  \alpha & \sim N(0, 10)\\
  \sigma &\sim \text{HalfCauchy}(0, 10)\\
  \beta & \sim N(0, 10)
\end{align}




```{r, eval = F, fig.cap="Number of Survey Responses by Country", fig.pos="ht!"}
dt %>%
  group_by(country, round, round_year) %>%
  tally %>%
  ungroup() %>%
  mutate(country = fct_reorder(country, n)) %>%
  mutate(label = paste0(round, " (", round_year, ")")) %>%
  ggplot(aes(country, n, fill = label)) +
  geom_bar(stat = "identity") +
  scale_fill_grey(start = .8, end = .4) +
  coord_flip() +
  labs(x = "", y = "")
```


<!-- $$\pi(y,x|\theta) = \pi(y|x, \theta) $$ -->
<!-- this cancelles out to  -->
<!-- $$\pi(y, x|\theta) \approx \pi(y|x, \theta) $$ -->
<!-- now we plug in a binomial distribution.  -->
<!-- $$\pi(y|x, \theta) = Bin(y|f(x, \theta), N)$$ -->